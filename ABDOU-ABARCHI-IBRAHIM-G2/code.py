# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B1ltYuU8pclGKqaYxopGWcJoMW2TzHHw

# INFORMATION RETRIEVAL
"""

# To make this system, then it follows specific steps(processes)

# ignore all warnings
import warnings
warnings.filterwarnings('ignore')

"""# Loading libraries"""

import regex as re  
import json
import unidecode
import string
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import collections
import math
import xml.etree.ElementTree as ET
import pandas as pd

pd.set_option('display.max_colwidth', -1)

"""# <<<<<<<<<< Data Selection>>>>>>>>>

# read in general file types
"""

def in_data(filename, dtype, reg):
    with open(filename, 'r') as freader:
        if dtype == 'ind': # in index
            data = json.load(freader)
        elif dtype == 'xml': # read in xml files
            tree = ET.parse(filename)
            root = tree.getroot()
            data = []
            for doc in root.findall('DOC'):
                n_doc = doc.find('DOCNO').text
                headline = doc.find('HEADLINE').text
                txr = doc.find('TEXT').text
                new_text = headline + txr
                data.append([n_doc,new_text])
        else: # read text files
            data = re.findall(reg, freader.read())
    return data

"""# write out to file"""

def out_data(data, filename, dtype, query, rnk):
    with open(filename, 'w') as freader:
        if dtype == 'prep': # write out data, preprocessed
            for var in data:
                freader.write(var + '\n')
        elif dtype =='fnd': # index in text for visualization
            for (key, value) in data.items():
                freader.write(key + ': ')
                for (dkey, dvalue) in value.items():
                    freader.write('\n\t' + dkey + ': ')
                    temp = ', '.join(str(pos) for pos in dvalue)
                    freader.write(temp)
                freader.write('\n')
        elif dtype == 'ind': # write index in json file
            json.dump(data, freader)

def results_out(data, filename, query, rnk):
    with open(filename, 'a+') as freader:
        if not data:
                freader.write(str(query) + ', ' + 'No documents found\n\n')
        else:
            if rnk: # if ranking is involved 
                i = 0
                for key, value in data.items():
                    if i<150:
                        freader.write(str(query) + ', ' + key + ', ' + str(value) + '\n')
                        i += 1
                freader.write('\n\n')
                
            else:
                for var in data:
                    freader.write(str(query) + ', ' + var + '\n')
                freader.write('\n\n')

"""# <<<<<<<<<< Preprocessing>>>>>>>>>"""

def sub_special(text, spec):
    text_out = re.sub(spec[0], spec[1], text)
    return text_out

def preprocess_d(sentence, stop_words, preping):
    sentence = sub_special(sentence, [' +', ' '])
    sentence = sub_special(sentence, ['FT', ''])
    sentence = sub_special(sentence, ['/ (CORRECTED)', ''])
    sentence = sub_special(sentence, ['\n', ' '])
    sentence = unidecode.unidecode(sentence)
    sentence = sentence.lower()
    sentence = re.sub(r'\p{P}(?<!-)','', sentence)# eliminating punctuations
    sentence = re.sub(r'-\s', ' ',sentence)
    sentence = re.sub(r'\s-', ' ',sentence)
    sentence = re.sub('\s-*\s*', ' ',sentence)
    sentence = re.sub('\d+\s', ' ', sentence)
    sentence = re.sub(r'[+]',' ', sentence)
    sentence = re.sub(r' +', ' ', sentence)
    sentence = sentence.strip()
    words = word_tokenize(sentence)
    if preping:
        filtered = [PorterStemmer().stem(word) for word in words if word not in stop_words]
    else:
        filtered = words
    new_sentence = ' '.join(filtered)
    return new_sentence

"""# <<<<<<<<<< Indexing>>>>>>>>>"""

def string_to_array(sentence, sepr):
    sentence = sub_special(sentence, [' +', ' '])
    sentence = sentence.strip()
    tlist = list(sentence.split(sepr))
    return tlist

def doc_ref(prep_data):
    doc_dict = {}
    for docsent in prep_data:
        ldoc = string_to_array(docsent[1], ' ')
        doc_dict[docsent[0]] = len(ldoc)
    return doc_dict

def term_maker(sentence):
    terms = []
    tlist = string_to_array(sentence, ' ')
    for term in tlist:
        if term not in terms:
            terms.append(term)
    return terms

def term_position(sentence, term):
    tlist = string_to_array(sentence, ' ')
    result = [ pos + 1 for pos, val in enumerate(tlist) if val ==term]
    return result

def termlist(in_data):
    terms = []
    for sentence in in_data:
        for term in term_maker(sentence[1]):
            if term not in terms:
                terms.append(term)
    terms.sort()
    return terms

def index_data(in_data, terms):
    docterm_dict = {}
    iter_dict = {}
    for term in terms:
        for doc_sent in in_data:
            term_pos = term_position(doc_sent[1], term)
            if len(term_pos) != 0:
                docterm_dict[doc_sent[0]] = term_pos
        iter_dict[term] = docterm_dict.copy()
        docterm_dict.clear()
    return iter_dict

"""# Search methods"""

def freq_count(array):
    return collections.Counter(array)

def word_search(dic, term, rnk):
    result = [False]
    if rnk == 'gen':
        for key, value in dic.items():
            if term == key:
                result[0] = True
                result.append(list(value.keys()))
                break
    elif rnk == 'proxy' or rnk == 'wrank':
        for key, value in dic.items():
            if term == key:
                result[0] = True
                result.append(value)
                break
    return result

def phrasal_search(query, stopwords, p_index):
    nlist = []
    result = False
    qsentence = preprocess_d(query, stopwords, True)
    qv = string_to_array(qsentence, ' ')
    result, nlist = prox_contain(qv, stopwords, p_index, False)
    return [result, nlist, qv]

def or_opr(query):
    or_op = re.compile(r'(.+) OR (.+)').search(query)
    return or_op


def prox_sr(query):
    prox = re.compile(r'#(\d+)\((\w+), (\w+)\)').search(query)
    if not prox:
        prox = re.compile(r'#(\d+)\((\w+),(\w+)\)').search(query)
    return prox


def phras_f(query):
    phr = re.compile(r'"(.+)"').search(query)
    return phr


def and_opr(query):
    and_r = re.compile(r'(.+) AND (.+)').search(query)
    return and_r


def not_opf(query):
    not_r = re.compile(r'NOT (.+)').search(query)
    return not_r

def and_contain(qvector, stopwords, p_index, ldoc):
    nlist = [False]
    l_qv = []
    ldoc.sort()
    for var in qvector:
        not_op = not_opf(var)
        phras = phras_f(var)
        if not_op:
            ntemp = preprocess_d(not_op.group(), stopwords, True)
            rtemp = ""
            if ntemp:
                rtemp = word_search(p_index, ntemp, 'gen')
            else:
                nlist[0] = False
                break
            if rtemp[0]:
                nlist[0] = True
                for doc in ldoc:
                    if doc not in rtemp[1]:
                        l_qv.append(doc)
            else:
                nlist[0] = False
                break
        elif phras:
            result, ldoc_, qvector = phrasal_search(phras.group(), stopwords, p_index)
            if result:
                if ldoc_:
                    nlist[0] = True
                    for items in ldoc_:
                        l_qv.append(items)
            else:
                nlist[0] = False
                break
        else:
            temp = preprocess_d(var, stopwords, True)
            rtemp = ""
            if temp:
                rtemp = word_search(p_index, temp, 'gen')
            else:
                pass
            if rtemp[0]:
                nlist[0] = True
                for items in rtemp[1]:
                    l_qv.append(items)
            else:
                nlist[0] = False
                break
    nlist.append(l_qv)
    return nlist

def or_contain(qvector, stopwords, p_index, ldoc):
    found = False
    nlist = []
    for var in qvector:
        not_op = not_opf(var)
        phras = phras_f(var)
        if not_op:
            ntemp = preprocess_d(not_op.group(), stopwords, True)
            rtemp = ""
            if ntemp:
                rtemp = word_search(p_index, ntemp, 'gen')
            else:
                continue
            if rtemp[0]:
                nlist[0] = True
                for doc in ldoc:
                    if doc not in rtemp[1]:
                        nlist.append(doc)
        elif phras:
            result, ldoc_, qvector = phrasal_search(phras.group(), stopwords, p_index)
            if ldoc_:
                found = True
                for items in ldoc_:
                    nlist.append(items)
            else:
                found = False
                break

        else:
            temp = preprocess_d(var, stopwords, True)
            rtemp = ""
            if temp:
                rtemp = word_search(p_index, temp, 'gen')
            else:
                continue
            if rtemp[0]:
                found = True
                for items in rtemp[1]:
                    nlist.append(items)
            else:
                pass
    if nlist:
        nalist = [eval(var) for var in nlist]
        nalist = [str(var) for var in nalist]
    else:
        nalist = []

    return [found, nalist]

def contain_not(query, stopwords, p_index, doclist):
    phrase = phras_f(query)
    prox = prox_sr(query)
    rdoclist = []
    if phrase:
        result, nlist, qvector = phrasal_search(phrase.group(), stopwords, p_index)
        if result:
            rdic = freq_count(nlist)
            temp = disply_result(rdic, len(qvector))
            for doc in doclist:
                if doc not in temp:
                    rdoclist.append(doc)
        else:
            rdoclist = []
    elif prox:
        result, temp = prox_contain(prox.groups(), stopwords, p_index, True)
        if not result:
            rdoclist = []
        else:
            for doc in doclist:
                if doc not in temp:
                    rdoclist.append(doc)
    else:
        qun = preprocess_d(query, stopwords, True)
        result = word_search(p_index, qun, 'gen')
        if not result[0]:
            rdoclist = []
        else:
            for doc in doclist:
                if doc not in result[1]:
                    rdoclist.append(doc)
    return rdoclist

def prox_contain(query, stopwords, p_index, spec):
    if spec:
        qv = query[1:]
        dif = query[0]
    else:
        qv = query
        dif = 1
    nlist = []
    result = False
    for var in range(len(qv) - 1):
        lw = qv[var]
        rw = qv[var + 1]
        lwn = preprocess_d(lw, stopwords, True)
        rwn = preprocess_d(rw, stopwords, True)
        if rwn and lwn:
            lwr = word_search(p_index, lwn, 'proxy')
            rwr = word_search(p_index, rwn, 'proxy')

            if lwr[0] and rwr[0]:
                rkeyl = list(rwr[1].keys())
                lkeyl = list(lwr[1].keys())
                for rvar in rkeyl:
                    for lvar in lkeyl:
                        if rvar == lvar:
                            rtempl = rwr[1][rvar]
                            ltempl = lwr[1][lvar]
                            for rkey in rtempl:
                                for lkey in ltempl:
                                    tresu = rkey - lkey
                                    if spec:
                                        if 0 < tresu <= int(dif):
                                            result = True
                                            nlist.append(rvar)
                                            break
                                    else:
                                        if tresu == 1:
                                            result = True
                                            nlist.append(rvar)
                                            break
                            break
                                        
                                            
                                    
                                
    return [result, nlist]

"""# Display functions"""

def disply_result(dic, larg): # documents with frequence == no terms
    ldoc = []
    for (key, value) in dic.items():
        if value == larg:
            ldoc.append(key)
    return ldoc

def disply_or(dic, exis):
    ldoc = []
    if exis:
        for (key, value) in dic.items():
            ldoc.append(key)
    return ldoc

def sortDict(rdict):
    result_dict = {}
    vlist = list(rdict.values())
    vlist.sort(reverse=True)
    for var in vlist:
        for key, value in rdict.items():
            if value == var:
                result_dict[key] = var
    return result_dict

"""# <<<<<<<<<< Models>>>>>>>>>

# Boolean search
"""

def boolean_search(query, stopwords, p_index, ldoc):
    query = re.sub(' +', ' ', query)
    nlist = []
    and_op = and_opr(query)
    or_op = or_opr(query)
    prox_s = prox_sr(query)
    phra = phras_f(query)
    cont_not = not_opf(query)
    ldoc_ = ""
    if and_op:
        ldoc_ = ""
        nlist = and_contain(and_op.groups(), stopwords, p_index, ldoc)
        if nlist[0]:
            rdic = freq_count(nlist[1])
            ldoc_ = disply_result(rdic, len(and_op.groups()))
        else:
            ldoc_ = []
    elif or_op:
        nlist = or_contain(or_op.groups(), stopwords, p_index, ldoc)
        if nlist[0]:
            rdic = freq_count(nlist[1])
            if rdic:
                ldoc_ = disply_or(rdic, True)

        else:
            ldoc_ = []
    elif prox_s:
        result, ldoc_ = prox_contain(prox_s.groups(), stopwords, p_index, True)
        if not result:
            ldoc_ = []
    elif phra:
        result, nlist, qvector = phrasal_search(phra.group(), stopwords, p_index)
        if result:
            ldoc_ = []
            for var in nlist:
                if var not in ldoc_:
                    ldoc_.append(var)

        else:
            ldoc_ = []
    elif cont_not:
        ldoc_ = ''
        ldoc_ = contain_not(not_op.group(), stopwords, p_index, ldoc)
    else:
        templ = []
        qun = preprocess_d(query, stopwords, True)
        result = word_search(p_index, qun, 'gen')
        if result[0]:
            ldoc_ = result[1]
    nalist = [eval(vart) for vart in ldoc_]
    nalist.sort()
    ldoc_ = [str(vart) for vart in nalist]
    return ldoc_

"""# Ranked Retrieval TFIDF model"""

def tfidf_index(index_p, terms, doc_len_c):
    ranking_dict = {}
    doc_array = list(doc_len_c.keys())
    # calculate the weights
    for key, value in index_p.items():
        dft = len(list(value.keys()))
        inner_dic = {}
        for keys, values in value.items():
            tfd = len(values)
            wtd = (1 + math.log10(tfd)) * math.log10(len(doc_array) / dft)
            inner_dic[keys] = (tfd, wtd)

        ranking_dict[key] = [dft, inner_dic]
    return ranking_dict

def cosine_similarity(in_dict):
    docdot = {}
    modularq = {}
    modulard = {}
    for key, value in in_dict.items():
        lenq = 0
        lend = 0
        for keys, values in value.items():
            dp = values[0] * values[1]
            lenq += values[0]**2
            lend += values[1]**2
            if keys in list(modularq.keys()):
                modularq[keys] += lenq
            else:
                modularq[keys] = lenq
            if keys in list(modulard.keys()):
                modulard[keys] += lend
            else:
                modulard[keys] = lend
            if keys in list(docdot.keys()):
                docdot[keys] += dp
            else:
                docdot[keys] = dp
    for key, value in modularq.items():
        modularq[key] = math.sqrt(modularq[key])
        
    for key, value in modulard.items():
        modulard[key] = math.sqrt(modulard[key])
    
    for key,value in docdot.items():
        docdot[key] = docdot[key]/ modularq[key] * modulard[key]
    return docdot

def query_treat(in_query, stopwords, index_p, doc_dict):
    qsentence = preprocess_d(in_query, stopwords, True)
    qvector = string_to_array(qsentence, ' ')
    qdic = freq_count(qvector)
    len_doc = list(doc_dict.keys())
    docvdic = {}
    iterdic = {}
    for vec in qvector:
        res = word_search(index_p, vec, 'wrank')
        if res[0]:
            rlist = res[1]
            iterdic.clear()
            for key, value in rlist[1].items():
                wtq = ((1 + math.log10(qdic[vec]))* math.log10((len(len_doc)/rlist[0])))
                wtd = value[1]
                iterdic[key] = [wtq, wtd]
            docvdic[vec] = iterdic.copy()
    #print(docvdic)
    d_score = cosine_similarity(docvdic)
    d_score = sortDict(d_score)
    return d_score

"""# TESTING THE CODE"""

# Acquisition

sampledoc = 'Datasets/RIW/in_data/trec.5000.xml'

stopwordf = 'Datasets/RIW/in_data/stopwords.txt'

stopwords = in_data(stopwordf, 'gen', '(.+)')

data = in_data(sampledoc, 'xml', '')

df = pd.DataFrame(data)

data_prep = []
for idata in data:
    ddata = preprocess_d(idata[1], stopwords, True)
    data_prep.append((idata[0], ddata))

df = pd.DataFrame(data_prep)

df.head()

"""# build the index dictionary"""

terms = termlist(data_prep)

index = index_data(data_prep, terms)

# writing out the index for visualization

inpath = 'Datasets/RIW/in_data/trec.5000.position.index.json'
outpath = 'Datasets/RIW/out_folder/index.txt'

outpd = 'Datasets/RIW/in_data/trec.5000.docref.json'

out_data(index, outpath, 'fnd', '','')

# writing index in json format

out_data(index, inpath, 'ind', '','')

# writing doc frequence out

docref = doc_ref(data_prep)

out_data(docref, outpd, 'ind', '', '')

"""# Boolean retrieval"""

# loading necessary documents

# loading index
index = in_data(inpath, 'ind', '')

docdict = in_data(outpd, 'ind', '') # loading the doc reference

# loading query

filename = "Datasets/RIW/in_data/queries.boolean.txt"
resultf = "Datasets/RIW/out_folder/results.boolean.txt"
reg = '(\d+) (.+)'



rfilename = "Datasets/RIW/in_data/queries.ranked.txt"
rreg = '(\d+) (.+)'
resultrf = "Datasets/RIW/out_folder/results.ranked.txt"

# testing with coursework queries

queries = in_data(filename, 'gen', reg)

df = pd.DataFrame(queries)

df

rqueries = in_data(rfilename, 'gen', rreg)

df = pd.DataFrame(rqueries)

df



doclist = list(docdict.keys())

len(doclist)

result = boolean_search(queries[9][1], stopwords, index, doclist)

for query in queries:
    result = boolean_search(query[1], stopwords, index, doclist)
    results_out(result, resultf, query[0], False)

"""# Ranked Retrieval"""

r_index = tfidf_index(index, terms, docdict)

for query in rqueries:
    rdict = query_treat(query[1], stopwords, r_index, docdict)
    results_out(rdict, resultrf, query[0], True)